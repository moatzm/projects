{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d23ce583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\moatz\\Desktop\\Customer Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50a67a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call  Failure</th>\n",
       "      <th>Complains</th>\n",
       "      <th>Subscription  Length</th>\n",
       "      <th>Charge  Amount</th>\n",
       "      <th>Seconds of Use</th>\n",
       "      <th>Frequency of use</th>\n",
       "      <th>Frequency of SMS</th>\n",
       "      <th>Distinct Called Numbers</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Tariff Plan</th>\n",
       "      <th>Status</th>\n",
       "      <th>Age</th>\n",
       "      <th>Customer Value</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>4370</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>197.640</td>\n",
       "      <td>177.8760</td>\n",
       "      <td>69.7640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>46.035</td>\n",
       "      <td>41.4315</td>\n",
       "      <td>60.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2453</td>\n",
       "      <td>60</td>\n",
       "      <td>359</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1536.520</td>\n",
       "      <td>1382.8680</td>\n",
       "      <td>203.6520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>4198</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>240.020</td>\n",
       "      <td>216.0180</td>\n",
       "      <td>74.0020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2393</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>145.805</td>\n",
       "      <td>131.2245</td>\n",
       "      <td>64.5805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>6697</td>\n",
       "      <td>147</td>\n",
       "      <td>92</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>721.980</td>\n",
       "      <td>649.7820</td>\n",
       "      <td>122.1980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>9237</td>\n",
       "      <td>177</td>\n",
       "      <td>80</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>261.210</td>\n",
       "      <td>235.0890</td>\n",
       "      <td>76.1210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>3157</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>280.320</td>\n",
       "      <td>252.2880</td>\n",
       "      <td>78.0320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4695</td>\n",
       "      <td>46</td>\n",
       "      <td>222</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1077.640</td>\n",
       "      <td>969.8760</td>\n",
       "      <td>157.7640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1792</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>100.680</td>\n",
       "      <td>90.6120</td>\n",
       "      <td>60.0680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Call  Failure  Complains  Subscription  Length  Charge  Amount  \\\n",
       "0                 8          0                    38               0   \n",
       "1                 0          0                    39               0   \n",
       "2                10          0                    37               0   \n",
       "3                10          0                    38               0   \n",
       "4                 3          0                    38               0   \n",
       "...             ...        ...                   ...             ...   \n",
       "3145             21          0                    19               2   \n",
       "3146             17          0                    17               1   \n",
       "3147             13          0                    18               4   \n",
       "3148              7          0                    11               2   \n",
       "3149              8          1                    11               2   \n",
       "\n",
       "      Seconds of Use  Frequency of use  Frequency of SMS  \\\n",
       "0               4370                71                 5   \n",
       "1                318                 5                 7   \n",
       "2               2453                60               359   \n",
       "3               4198                66                 1   \n",
       "4               2393                58                 2   \n",
       "...              ...               ...               ...   \n",
       "3145            6697               147                92   \n",
       "3146            9237               177                80   \n",
       "3147            3157                51                38   \n",
       "3148            4695                46               222   \n",
       "3149            1792                25                 7   \n",
       "\n",
       "      Distinct Called Numbers  Age Group  Tariff Plan  Status  Age  \\\n",
       "0                          17          3            1       1   30   \n",
       "1                           4          2            1       2   25   \n",
       "2                          24          3            1       1   30   \n",
       "3                          35          1            1       1   15   \n",
       "4                          33          1            1       1   15   \n",
       "...                       ...        ...          ...     ...  ...   \n",
       "3145                       44          2            2       1   25   \n",
       "3146                       42          5            1       1   55   \n",
       "3147                       21          3            1       1   30   \n",
       "3148                       12          3            1       1   30   \n",
       "3149                        9          3            1       1   30   \n",
       "\n",
       "      Customer Value         FN        FP  Churn  \n",
       "0            197.640   177.8760   69.7640      0  \n",
       "1             46.035    41.4315   60.0000      0  \n",
       "2           1536.520  1382.8680  203.6520      0  \n",
       "3            240.020   216.0180   74.0020      0  \n",
       "4            145.805   131.2245   64.5805      0  \n",
       "...              ...        ...       ...    ...  \n",
       "3145         721.980   649.7820  122.1980      0  \n",
       "3146         261.210   235.0890   76.1210      0  \n",
       "3147         280.320   252.2880   78.0320      0  \n",
       "3148        1077.640   969.8760  157.7640      0  \n",
       "3149         100.680    90.6120   60.0680      1  \n",
       "\n",
       "[3150 rows x 16 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ac01185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Call  Failure            3150 non-null   int64  \n",
      " 1   Complains                3150 non-null   int64  \n",
      " 2   Subscription  Length     3150 non-null   int64  \n",
      " 3   Charge  Amount           3150 non-null   int64  \n",
      " 4   Seconds of Use           3150 non-null   int64  \n",
      " 5   Frequency of use         3150 non-null   int64  \n",
      " 6   Frequency of SMS         3150 non-null   int64  \n",
      " 7   Distinct Called Numbers  3150 non-null   int64  \n",
      " 8   Age Group                3150 non-null   int64  \n",
      " 9   Tariff Plan              3150 non-null   int64  \n",
      " 10  Status                   3150 non-null   int64  \n",
      " 11  Age                      3150 non-null   int64  \n",
      " 12  Customer Value           3150 non-null   float64\n",
      " 13  FN                       3150 non-null   float64\n",
      " 14  FP                       3150 non-null   float64\n",
      " 15  Churn                    3150 non-null   int64  \n",
      "dtypes: float64(3), int64(13)\n",
      "memory usage: 393.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "507a08cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call  Failure              0\n",
       "Complains                  0\n",
       "Subscription  Length       0\n",
       "Charge  Amount             0\n",
       "Seconds of Use             0\n",
       "Frequency of use           0\n",
       "Frequency of SMS           0\n",
       "Distinct Called Numbers    0\n",
       "Age Group                  0\n",
       "Tariff Plan                0\n",
       "Status                     0\n",
       "Age                        0\n",
       "Customer Value             0\n",
       "FN                         0\n",
       "FP                         0\n",
       "Churn                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67ee0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_val_score,learning_curve, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea50d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "inputs=df.drop(columns=['Churn'])\n",
    "target=df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb6af417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "inputs_scaled=scaler.fit_transform(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48c9a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for the split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the variables with an 80-20 split and some random state\n",
    "# To have the same split as mine, use random_state = 365\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, target, test_size=0.2, random_state=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6c48b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data: (2520, 15)\n",
      "Resampled training data: (4218, 15)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original training data: {x_train.shape}\")\n",
    "print(f\"Resampled training data: {X_train_res.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c55a23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Churn</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Churn  count\n",
       "0      0   2109\n",
       "1      1    411"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75eb2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e44df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8492063492063492\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91       546\n",
      "           1       0.47      0.90      0.62        84\n",
      "\n",
      "    accuracy                           0.85       630\n",
      "   macro avg       0.72      0.87      0.76       630\n",
      "weighted avg       0.91      0.85      0.87       630\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Predict class labels and probabilities\n",
    "y_pred = logreg.predict(x_test)\n",
    "y_proba = logreg.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# 4. Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Rain\", \"Rain\"], yticklabels=[\"No Rain\", \"Rain\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51238628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best cross-validation score: 0.87\n",
      "Test accuracy: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "50 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.85063781 0.86770533        nan        nan 0.87268669 0.86770814\n",
      "        nan        nan 0.86747286 0.86439004        nan        nan\n",
      " 0.86320549 0.86344246        nan        nan 0.86273127 0.86249431\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l2', 'none'],  # Regularization term (L2 regularization or none)\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],  # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Solver for optimization\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Make predictions using the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ef5137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7986 - loss: 1.0912 - val_accuracy: 0.8945 - val_loss: 0.5389\n",
      "Epoch 2/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8581 - loss: 0.4006 - val_accuracy: 0.9360 - val_loss: 0.4374\n",
      "Epoch 3/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8689 - loss: 0.3386 - val_accuracy: 0.9419 - val_loss: 0.3165\n",
      "Epoch 4/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8679 - loss: 0.3186 - val_accuracy: 0.9443 - val_loss: 0.2873\n",
      "Epoch 5/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8817 - loss: 0.2871 - val_accuracy: 0.9668 - val_loss: 0.2555\n",
      "Epoch 6/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8794 - loss: 0.2796 - val_accuracy: 0.9123 - val_loss: 0.3947\n",
      "Epoch 7/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8937 - loss: 0.2757 - val_accuracy: 0.9645 - val_loss: 0.2828\n",
      "Epoch 8/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9108 - loss: 0.2508 - val_accuracy: 0.9562 - val_loss: 0.2419\n",
      "Epoch 9/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9117 - loss: 0.2442 - val_accuracy: 0.9028 - val_loss: 0.3433\n",
      "Epoch 10/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9200 - loss: 0.2491 - val_accuracy: 0.9573 - val_loss: 0.2052\n",
      "Epoch 11/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9365 - loss: 0.2150 - val_accuracy: 0.9645 - val_loss: 0.2013\n",
      "Epoch 12/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9356 - loss: 0.2142 - val_accuracy: 0.9348 - val_loss: 0.2976\n",
      "Epoch 13/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9333 - loss: 0.2170 - val_accuracy: 0.9562 - val_loss: 0.1971\n",
      "Epoch 14/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9389 - loss: 0.1996 - val_accuracy: 0.9751 - val_loss: 0.1636\n",
      "Epoch 15/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9401 - loss: 0.1937 - val_accuracy: 0.9550 - val_loss: 0.2249\n",
      "Epoch 16/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9381 - loss: 0.1960 - val_accuracy: 0.9502 - val_loss: 0.2124\n",
      "Epoch 17/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9444 - loss: 0.1908 - val_accuracy: 0.9419 - val_loss: 0.2352\n",
      "Epoch 18/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9487 - loss: 0.1810 - val_accuracy: 0.9562 - val_loss: 0.2216\n",
      "Epoch 19/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9477 - loss: 0.1811 - val_accuracy: 0.9597 - val_loss: 0.1805\n",
      "Epoch 20/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9500 - loss: 0.1690 - val_accuracy: 0.9573 - val_loss: 0.2029\n",
      "Epoch 21/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9445 - loss: 0.1795 - val_accuracy: 0.9822 - val_loss: 0.1258\n",
      "Epoch 22/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1767 - val_accuracy: 0.9905 - val_loss: 0.1084\n",
      "Epoch 23/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9349 - loss: 0.1924 - val_accuracy: 0.9538 - val_loss: 0.1807\n",
      "Epoch 24/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9510 - loss: 0.1713 - val_accuracy: 0.9834 - val_loss: 0.1387\n",
      "Epoch 25/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9429 - loss: 0.1836 - val_accuracy: 0.9799 - val_loss: 0.1125\n",
      "Epoch 26/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9487 - loss: 0.1751 - val_accuracy: 0.9846 - val_loss: 0.1145\n",
      "Epoch 27/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9512 - loss: 0.1702 - val_accuracy: 0.9905 - val_loss: 0.1314\n",
      "Epoch 28/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9550 - loss: 0.1626 - val_accuracy: 0.9905 - val_loss: 0.0872\n",
      "Epoch 29/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9507 - loss: 0.1824 - val_accuracy: 0.9858 - val_loss: 0.1214\n",
      "Epoch 30/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1473 - val_accuracy: 0.9562 - val_loss: 0.1908\n",
      "Epoch 31/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.1724 - val_accuracy: 0.9751 - val_loss: 0.1389\n",
      "Epoch 32/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9544 - loss: 0.1599 - val_accuracy: 0.9348 - val_loss: 0.2306\n",
      "Epoch 33/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.1544 - val_accuracy: 0.9799 - val_loss: 0.1204\n",
      "Epoch 34/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9520 - loss: 0.1641 - val_accuracy: 0.9893 - val_loss: 0.1112\n",
      "Epoch 35/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9620 - loss: 0.1402 - val_accuracy: 0.8341 - val_loss: 0.3579\n",
      "Epoch 36/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9514 - loss: 0.1584 - val_accuracy: 0.9810 - val_loss: 0.1503\n",
      "Epoch 37/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9543 - loss: 0.1573 - val_accuracy: 0.9929 - val_loss: 0.0965\n",
      "Epoch 38/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9533 - loss: 0.1539 - val_accuracy: 0.9704 - val_loss: 0.1819\n",
      "Epoch 39/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9630 - loss: 0.1457 - val_accuracy: 0.9893 - val_loss: 0.1045\n",
      "Epoch 40/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9544 - loss: 0.1498 - val_accuracy: 0.9917 - val_loss: 0.0874\n",
      "Epoch 41/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9569 - loss: 0.1472 - val_accuracy: 0.9846 - val_loss: 0.1389\n",
      "Epoch 42/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.1399 - val_accuracy: 0.9870 - val_loss: 0.1196\n",
      "Epoch 43/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9593 - loss: 0.1520 - val_accuracy: 0.9739 - val_loss: 0.1484\n",
      "Epoch 44/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9551 - loss: 0.1569 - val_accuracy: 0.9834 - val_loss: 0.1388\n",
      "Epoch 45/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1584 - val_accuracy: 0.9810 - val_loss: 0.1178\n",
      "Epoch 46/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9615 - loss: 0.1412 - val_accuracy: 0.9562 - val_loss: 0.1809\n",
      "Epoch 47/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9500 - loss: 0.1663 - val_accuracy: 0.9443 - val_loss: 0.2096\n",
      "Epoch 48/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1458 - val_accuracy: 0.9775 - val_loss: 0.1424\n",
      "Epoch 49/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9529 - loss: 0.1623 - val_accuracy: 0.9870 - val_loss: 0.1207\n",
      "Epoch 50/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9631 - loss: 0.1388 - val_accuracy: 0.9834 - val_loss: 0.1270\n",
      "Epoch 51/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9502 - loss: 0.1606 - val_accuracy: 0.9846 - val_loss: 0.1296\n",
      "Epoch 52/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1504 - val_accuracy: 0.9194 - val_loss: 0.2638\n",
      "Epoch 53/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9579 - loss: 0.1513 - val_accuracy: 0.9751 - val_loss: 0.1225\n",
      "Epoch 54/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9577 - loss: 0.1442 - val_accuracy: 0.9585 - val_loss: 0.1783\n",
      "Epoch 55/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9586 - loss: 0.1491 - val_accuracy: 0.9633 - val_loss: 0.1434\n",
      "Epoch 56/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9592 - loss: 0.1440 - val_accuracy: 0.9799 - val_loss: 0.1366\n",
      "Epoch 57/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1459 - val_accuracy: 0.9905 - val_loss: 0.0959\n",
      "Epoch 58/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1445 - val_accuracy: 0.9680 - val_loss: 0.1622\n",
      "Epoch 59/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9557 - loss: 0.1431 - val_accuracy: 0.9491 - val_loss: 0.1849\n",
      "Epoch 60/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9561 - loss: 0.1540 - val_accuracy: 0.9822 - val_loss: 0.1051\n",
      "Epoch 61/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9540 - loss: 0.1554 - val_accuracy: 0.9893 - val_loss: 0.0911\n",
      "Epoch 62/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.1474 - val_accuracy: 0.9467 - val_loss: 0.1898\n",
      "Epoch 63/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9584 - loss: 0.1516 - val_accuracy: 0.9929 - val_loss: 0.1061\n",
      "Epoch 64/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1458 - val_accuracy: 0.9917 - val_loss: 0.0984\n",
      "Epoch 65/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9609 - loss: 0.1434 - val_accuracy: 0.9822 - val_loss: 0.1141\n",
      "Epoch 66/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1439 - val_accuracy: 0.9775 - val_loss: 0.1198\n",
      "Epoch 67/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9621 - loss: 0.1390 - val_accuracy: 0.9538 - val_loss: 0.1678\n",
      "Epoch 68/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9589 - loss: 0.1485 - val_accuracy: 0.9751 - val_loss: 0.1283\n",
      "Epoch 69/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9555 - loss: 0.1541 - val_accuracy: 0.9822 - val_loss: 0.1346\n",
      "Epoch 70/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9584 - loss: 0.1516 - val_accuracy: 0.9704 - val_loss: 0.1410\n",
      "Epoch 71/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9545 - loss: 0.1498 - val_accuracy: 0.9905 - val_loss: 0.1048\n",
      "Epoch 72/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9569 - loss: 0.1485 - val_accuracy: 0.9846 - val_loss: 0.1151\n",
      "Epoch 73/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9586 - loss: 0.1432 - val_accuracy: 0.9597 - val_loss: 0.1875\n",
      "Epoch 74/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.1299 - val_accuracy: 0.9870 - val_loss: 0.1198\n",
      "Epoch 75/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.1331 - val_accuracy: 0.9716 - val_loss: 0.1391\n",
      "Epoch 76/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9544 - loss: 0.1522 - val_accuracy: 0.9633 - val_loss: 0.1532\n",
      "Epoch 77/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9609 - loss: 0.1388 - val_accuracy: 0.9301 - val_loss: 0.2450\n",
      "Epoch 78/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1467 - val_accuracy: 0.9953 - val_loss: 0.0764\n",
      "Epoch 79/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.1397 - val_accuracy: 0.9846 - val_loss: 0.1245\n",
      "Epoch 80/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1316 - val_accuracy: 0.9799 - val_loss: 0.1352\n",
      "Epoch 81/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.1304 - val_accuracy: 0.9822 - val_loss: 0.1168\n",
      "Epoch 82/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9676 - loss: 0.1274 - val_accuracy: 0.9834 - val_loss: 0.1174\n",
      "Epoch 83/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.1269 - val_accuracy: 0.9929 - val_loss: 0.0977\n",
      "Epoch 84/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.1457 - val_accuracy: 0.9905 - val_loss: 0.0861\n",
      "Epoch 85/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9652 - loss: 0.1238 - val_accuracy: 0.9799 - val_loss: 0.1569\n",
      "Epoch 86/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9588 - loss: 0.1414 - val_accuracy: 0.9858 - val_loss: 0.1126\n",
      "Epoch 87/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9552 - loss: 0.1423 - val_accuracy: 0.9834 - val_loss: 0.1181\n",
      "Epoch 88/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9610 - loss: 0.1399 - val_accuracy: 0.9822 - val_loss: 0.1330\n",
      "Epoch 89/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9621 - loss: 0.1373 - val_accuracy: 0.9810 - val_loss: 0.1022\n",
      "Epoch 90/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9595 - loss: 0.1440 - val_accuracy: 0.9893 - val_loss: 0.1092\n",
      "Epoch 91/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9600 - loss: 0.1426 - val_accuracy: 0.9905 - val_loss: 0.0934\n",
      "Epoch 92/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9606 - loss: 0.1414 - val_accuracy: 0.9751 - val_loss: 0.1306\n",
      "Epoch 93/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9567 - loss: 0.1485 - val_accuracy: 0.9384 - val_loss: 0.2135\n",
      "Epoch 94/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.1342 - val_accuracy: 0.9751 - val_loss: 0.1610\n",
      "Epoch 95/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9616 - loss: 0.1406 - val_accuracy: 0.9834 - val_loss: 0.1284\n",
      "Epoch 96/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9626 - loss: 0.1402 - val_accuracy: 0.9325 - val_loss: 0.1988\n",
      "Epoch 97/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9614 - loss: 0.1375 - val_accuracy: 0.9751 - val_loss: 0.1455\n",
      "Epoch 98/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9572 - loss: 0.1452 - val_accuracy: 0.9787 - val_loss: 0.1275\n",
      "Epoch 99/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9658 - loss: 0.1302 - val_accuracy: 0.9953 - val_loss: 0.0732\n",
      "Epoch 100/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9638 - loss: 0.1375 - val_accuracy: 0.9716 - val_loss: 0.1579\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Accuracy on test set: 96.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "model = Sequential()\n",
    "model.add(Dense(123, input_shape=(X_train_res.shape[1],), activation=\"relu\",kernel_regularizer=regularizers.l2(0.01)))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(56, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16, activation=\"relu\"))  \n",
    "model.add(Dense(8, activation=\"relu\"))   \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))                               # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_res, y_train_res, epochs=100, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdc7df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0a1faf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7817 - loss: 0.4366 - val_accuracy: 0.8922 - val_loss: 0.3445\n",
      "Epoch 2/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8831 - loss: 0.2410 - val_accuracy: 0.9372 - val_loss: 0.2445\n",
      "Epoch 3/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9142 - loss: 0.1938 - val_accuracy: 0.9585 - val_loss: 0.2111\n",
      "Epoch 4/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9270 - loss: 0.1725 - val_accuracy: 0.9787 - val_loss: 0.1622\n",
      "Epoch 5/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9472 - loss: 0.1396 - val_accuracy: 0.9597 - val_loss: 0.1666\n",
      "Epoch 6/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9445 - loss: 0.1345 - val_accuracy: 0.9716 - val_loss: 0.1307\n",
      "Epoch 7/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9580 - loss: 0.1175 - val_accuracy: 0.9882 - val_loss: 0.0826\n",
      "Epoch 8/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9560 - loss: 0.1099 - val_accuracy: 0.9799 - val_loss: 0.0870\n",
      "Epoch 9/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9615 - loss: 0.1028 - val_accuracy: 0.9550 - val_loss: 0.1397\n",
      "Epoch 10/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9572 - loss: 0.1008 - val_accuracy: 0.9739 - val_loss: 0.0954\n",
      "Epoch 11/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.1034 - val_accuracy: 0.9917 - val_loss: 0.0599\n",
      "Epoch 12/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.0936 - val_accuracy: 0.9739 - val_loss: 0.1301\n",
      "Epoch 13/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9654 - loss: 0.0942 - val_accuracy: 0.9680 - val_loss: 0.0936\n",
      "Epoch 14/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9697 - loss: 0.0789 - val_accuracy: 0.9787 - val_loss: 0.0951\n",
      "Epoch 15/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9689 - loss: 0.0837 - val_accuracy: 0.9822 - val_loss: 0.0826\n",
      "Epoch 16/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9610 - loss: 0.0982 - val_accuracy: 0.9846 - val_loss: 0.0812\n",
      "Epoch 17/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0812 - val_accuracy: 0.9645 - val_loss: 0.1483\n",
      "Epoch 18/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9662 - loss: 0.0862 - val_accuracy: 0.9846 - val_loss: 0.0718\n",
      "Epoch 19/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9764 - loss: 0.0687 - val_accuracy: 0.8495 - val_loss: 0.2538\n",
      "Epoch 20/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9622 - loss: 0.0943 - val_accuracy: 0.9704 - val_loss: 0.1065\n",
      "Epoch 21/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9670 - loss: 0.0855 - val_accuracy: 0.9242 - val_loss: 0.1127\n",
      "Epoch 22/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9786 - loss: 0.0684 - val_accuracy: 0.9846 - val_loss: 0.0753\n",
      "Epoch 23/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9715 - loss: 0.0732 - val_accuracy: 0.9858 - val_loss: 0.0743\n",
      "Epoch 24/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9751 - loss: 0.0672 - val_accuracy: 0.9502 - val_loss: 0.1428\n",
      "Epoch 25/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.0808 - val_accuracy: 0.9929 - val_loss: 0.0272\n",
      "Epoch 26/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0745 - val_accuracy: 0.9882 - val_loss: 0.0529\n",
      "Epoch 27/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9777 - loss: 0.0658 - val_accuracy: 0.9905 - val_loss: 0.0698\n",
      "Epoch 28/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9734 - loss: 0.0746 - val_accuracy: 0.9870 - val_loss: 0.0868\n",
      "Epoch 29/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9703 - loss: 0.0739 - val_accuracy: 0.9810 - val_loss: 0.1079\n",
      "Epoch 30/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0689 - val_accuracy: 0.9893 - val_loss: 0.0550\n",
      "Epoch 31/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9728 - loss: 0.0699 - val_accuracy: 0.9692 - val_loss: 0.0978\n",
      "Epoch 32/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0741 - val_accuracy: 0.9929 - val_loss: 0.0347\n",
      "Epoch 33/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.0716 - val_accuracy: 0.9929 - val_loss: 0.0494\n",
      "Epoch 34/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0619 - val_accuracy: 0.9858 - val_loss: 0.0760\n",
      "Epoch 35/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0669 - val_accuracy: 0.9763 - val_loss: 0.0690\n",
      "Epoch 36/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0691 - val_accuracy: 0.9882 - val_loss: 0.0541\n",
      "Epoch 37/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9782 - loss: 0.0617 - val_accuracy: 0.9834 - val_loss: 0.0566\n",
      "Epoch 38/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9734 - loss: 0.0687 - val_accuracy: 0.9905 - val_loss: 0.0604\n",
      "Epoch 39/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0591 - val_accuracy: 0.9810 - val_loss: 0.0837\n",
      "Epoch 40/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0602 - val_accuracy: 0.9929 - val_loss: 0.0369\n",
      "Epoch 41/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9756 - loss: 0.0652 - val_accuracy: 0.9822 - val_loss: 0.0686\n",
      "Epoch 42/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9797 - loss: 0.0513 - val_accuracy: 0.9953 - val_loss: 0.0287\n",
      "Epoch 43/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9777 - loss: 0.0606 - val_accuracy: 0.9822 - val_loss: 0.0671\n",
      "Epoch 44/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.0607 - val_accuracy: 0.9941 - val_loss: 0.0513\n",
      "Epoch 45/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9746 - loss: 0.0600 - val_accuracy: 0.9964 - val_loss: 0.0368\n",
      "Epoch 46/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9729 - loss: 0.0669 - val_accuracy: 0.9751 - val_loss: 0.1071\n",
      "Epoch 47/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9808 - loss: 0.0533 - val_accuracy: 0.9905 - val_loss: 0.0362\n",
      "Epoch 48/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0641 - val_accuracy: 0.9893 - val_loss: 0.0540\n",
      "Epoch 49/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9803 - loss: 0.0570 - val_accuracy: 0.9573 - val_loss: 0.1620\n",
      "Epoch 50/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0810 - val_accuracy: 0.9870 - val_loss: 0.0673\n",
      "Epoch 51/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9801 - loss: 0.0524 - val_accuracy: 0.9941 - val_loss: 0.0325\n",
      "Epoch 52/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9797 - loss: 0.0526 - val_accuracy: 0.9929 - val_loss: 0.0543\n",
      "Epoch 53/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.0553 - val_accuracy: 0.9775 - val_loss: 0.1108\n",
      "Epoch 54/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0805 - val_accuracy: 0.9953 - val_loss: 0.0461\n",
      "Epoch 55/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.0494 - val_accuracy: 0.9953 - val_loss: 0.0475\n",
      "Epoch 56/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0477 - val_accuracy: 0.9905 - val_loss: 0.0558\n",
      "Epoch 57/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9839 - loss: 0.0427 - val_accuracy: 0.9941 - val_loss: 0.0514\n",
      "Epoch 58/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9803 - loss: 0.0529 - val_accuracy: 0.9941 - val_loss: 0.0543\n",
      "Epoch 59/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9827 - loss: 0.0531 - val_accuracy: 0.9858 - val_loss: 0.0536\n",
      "Epoch 60/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9831 - loss: 0.0500 - val_accuracy: 0.9917 - val_loss: 0.0752\n",
      "Epoch 61/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9828 - loss: 0.0502 - val_accuracy: 0.9953 - val_loss: 0.0327\n",
      "Epoch 62/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0498 - val_accuracy: 0.9964 - val_loss: 0.0386\n",
      "Epoch 63/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.0494 - val_accuracy: 0.9893 - val_loss: 0.0430\n",
      "Epoch 64/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9829 - loss: 0.0465 - val_accuracy: 0.9905 - val_loss: 0.0522\n",
      "Epoch 65/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.0532 - val_accuracy: 0.9953 - val_loss: 0.0411\n",
      "Epoch 66/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0534 - val_accuracy: 0.9953 - val_loss: 0.0394\n",
      "Epoch 67/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9800 - loss: 0.0529 - val_accuracy: 0.9656 - val_loss: 0.1159\n",
      "Epoch 68/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9829 - loss: 0.0448 - val_accuracy: 0.9893 - val_loss: 0.0657\n",
      "Epoch 69/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9782 - loss: 0.0552 - val_accuracy: 0.9929 - val_loss: 0.0463\n",
      "Epoch 70/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9786 - loss: 0.0583 - val_accuracy: 0.9858 - val_loss: 0.0510\n",
      "Epoch 71/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9868 - loss: 0.0376 - val_accuracy: 0.9929 - val_loss: 0.0411\n",
      "Epoch 72/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0533 - val_accuracy: 0.9917 - val_loss: 0.0486\n",
      "Epoch 73/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.0507 - val_accuracy: 0.9905 - val_loss: 0.0669\n",
      "Epoch 74/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9855 - loss: 0.0383 - val_accuracy: 0.9976 - val_loss: 0.0344\n",
      "Epoch 75/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9775 - loss: 0.0660 - val_accuracy: 0.9941 - val_loss: 0.0371\n",
      "Epoch 76/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9794 - loss: 0.0498 - val_accuracy: 0.9953 - val_loss: 0.0493\n",
      "Epoch 77/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9841 - loss: 0.0400 - val_accuracy: 0.9953 - val_loss: 0.0312\n",
      "Epoch 78/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0486 - val_accuracy: 0.9917 - val_loss: 0.0359\n",
      "Epoch 79/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.0454 - val_accuracy: 0.9929 - val_loss: 0.0295\n",
      "Epoch 80/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9782 - loss: 0.0523 - val_accuracy: 0.9964 - val_loss: 0.0155\n",
      "Epoch 81/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.0866 - val_accuracy: 0.9514 - val_loss: 0.2544\n",
      "Epoch 82/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9798 - loss: 0.0599 - val_accuracy: 0.9941 - val_loss: 0.0436\n",
      "Epoch 83/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0435 - val_accuracy: 0.9858 - val_loss: 0.0580\n",
      "Epoch 84/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9830 - loss: 0.0454 - val_accuracy: 0.9905 - val_loss: 0.0512\n",
      "Epoch 85/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9855 - loss: 0.0408 - val_accuracy: 0.9893 - val_loss: 0.0611\n",
      "Epoch 86/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0474 - val_accuracy: 0.9739 - val_loss: 0.1091\n",
      "Epoch 87/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9799 - loss: 0.0569 - val_accuracy: 0.9929 - val_loss: 0.0319\n",
      "Epoch 88/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.0487 - val_accuracy: 0.9941 - val_loss: 0.0501\n",
      "Epoch 89/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0490 - val_accuracy: 0.8768 - val_loss: 0.3142\n",
      "Epoch 90/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9724 - loss: 0.0816 - val_accuracy: 0.8602 - val_loss: 0.3599\n",
      "Epoch 91/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0706 - val_accuracy: 0.9953 - val_loss: 0.0300\n",
      "Epoch 92/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0412 - val_accuracy: 0.9964 - val_loss: 0.0282\n",
      "Epoch 93/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.0484 - val_accuracy: 0.9668 - val_loss: 0.0845\n",
      "Epoch 94/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9779 - loss: 0.0606 - val_accuracy: 0.9929 - val_loss: 0.0419\n",
      "Epoch 95/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.0460 - val_accuracy: 0.9929 - val_loss: 0.0457\n",
      "Epoch 96/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9783 - loss: 0.0485 - val_accuracy: 0.9953 - val_loss: 0.0442\n",
      "Epoch 97/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9819 - loss: 0.0442 - val_accuracy: 0.9953 - val_loss: 0.0308\n",
      "Epoch 98/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0502 - val_accuracy: 0.9870 - val_loss: 0.0517\n",
      "Epoch 99/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.0427 - val_accuracy: 0.9941 - val_loss: 0.0392\n",
      "Epoch 100/100\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9851 - loss: 0.0400 - val_accuracy: 0.9810 - val_loss: 0.0975\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy on test set: 96.35%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "model = Sequential()\n",
    "model.add(Dense(123, input_shape=(X_train_res.shape[1],), activation=\"relu\"))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(56, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))  \n",
    "model.add(Dense(8, activation=\"relu\"))   \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))                               # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_res, y_train_res, epochs=100, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96f9639a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Confusion Matrix:\n",
      "[[530  16]\n",
      " [  5  79]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       546\n",
      "           1       0.83      0.94      0.88        84\n",
      "\n",
      "    accuracy                           0.97       630\n",
      "   macro avg       0.91      0.96      0.93       630\n",
      "weighted avg       0.97      0.97      0.97       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(x_test) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0fd9d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Confusion Matrix:\n",
      "[[2047   62]\n",
      " [  67 2042]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      2109\n",
      "           1       0.97      0.97      0.97      2109\n",
      "\n",
      "    accuracy                           0.97      4218\n",
      "   macro avg       0.97      0.97      0.97      4218\n",
      "weighted avg       0.97      0.97      0.97      4218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predss = (model.predict(X_train_res) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train_res, y_predss))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train_res, y_predss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c5ef21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\moatz\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8739 - loss: 0.3105 - val_accuracy: 0.9067 - val_loss: 0.2252\n",
      "Epoch 2/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9054 - loss: 0.2176 - val_accuracy: 0.9167 - val_loss: 0.1940\n",
      "Epoch 3/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9114 - loss: 0.2020 - val_accuracy: 0.9286 - val_loss: 0.1775\n",
      "Epoch 4/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9201 - loss: 0.1921 - val_accuracy: 0.9226 - val_loss: 0.1818\n",
      "Epoch 5/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9256 - loss: 0.1720 - val_accuracy: 0.9266 - val_loss: 0.1814\n",
      "Epoch 6/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9306 - loss: 0.1563 - val_accuracy: 0.9206 - val_loss: 0.1653\n",
      "Epoch 7/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9160 - loss: 0.1637 - val_accuracy: 0.9206 - val_loss: 0.1968\n",
      "Epoch 8/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9389 - loss: 0.1356 - val_accuracy: 0.9306 - val_loss: 0.1538\n",
      "Epoch 9/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9494 - loss: 0.1314 - val_accuracy: 0.9325 - val_loss: 0.1468\n",
      "Epoch 10/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9318 - loss: 0.1449 - val_accuracy: 0.9385 - val_loss: 0.1466\n",
      "Epoch 11/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9465 - loss: 0.1310 - val_accuracy: 0.9306 - val_loss: 0.1469\n",
      "Epoch 12/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9409 - loss: 0.1326 - val_accuracy: 0.9504 - val_loss: 0.1338\n",
      "Epoch 13/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9468 - loss: 0.1323 - val_accuracy: 0.9325 - val_loss: 0.1778\n",
      "Epoch 14/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9442 - loss: 0.1190 - val_accuracy: 0.9325 - val_loss: 0.1590\n",
      "Epoch 15/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.1281 - val_accuracy: 0.9365 - val_loss: 0.1550\n",
      "Epoch 16/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9432 - loss: 0.1451 - val_accuracy: 0.9425 - val_loss: 0.1473\n",
      "Epoch 17/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9417 - loss: 0.1188 - val_accuracy: 0.9405 - val_loss: 0.1691\n",
      "Epoch 18/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9445 - loss: 0.1169 - val_accuracy: 0.9306 - val_loss: 0.1719\n",
      "Epoch 19/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9427 - loss: 0.1296 - val_accuracy: 0.9365 - val_loss: 0.1586\n",
      "Epoch 20/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9437 - loss: 0.1253 - val_accuracy: 0.9444 - val_loss: 0.1543\n",
      "Epoch 21/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9450 - loss: 0.1284 - val_accuracy: 0.9425 - val_loss: 0.1331\n",
      "Epoch 22/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9498 - loss: 0.1029 - val_accuracy: 0.9643 - val_loss: 0.1043\n",
      "Epoch 23/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9523 - loss: 0.1140 - val_accuracy: 0.9504 - val_loss: 0.1233\n",
      "Epoch 24/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9529 - loss: 0.1072 - val_accuracy: 0.9385 - val_loss: 0.1334\n",
      "Epoch 25/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9493 - loss: 0.1033 - val_accuracy: 0.9524 - val_loss: 0.1027\n",
      "Epoch 26/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9630 - loss: 0.0821 - val_accuracy: 0.9603 - val_loss: 0.1245\n",
      "Epoch 27/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9537 - loss: 0.0898 - val_accuracy: 0.9444 - val_loss: 0.1196\n",
      "Epoch 28/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9577 - loss: 0.0933 - val_accuracy: 0.9325 - val_loss: 0.2244\n",
      "Epoch 29/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.1281 - val_accuracy: 0.9583 - val_loss: 0.1279\n",
      "Epoch 30/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1056 - val_accuracy: 0.9643 - val_loss: 0.1037\n",
      "Epoch 31/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9483 - loss: 0.1023 - val_accuracy: 0.9643 - val_loss: 0.1026\n",
      "Epoch 32/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9599 - loss: 0.0938 - val_accuracy: 0.9603 - val_loss: 0.1015\n",
      "Epoch 33/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9598 - loss: 0.0941 - val_accuracy: 0.9603 - val_loss: 0.1150\n",
      "Epoch 34/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.0957 - val_accuracy: 0.9504 - val_loss: 0.1362\n",
      "Epoch 35/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.0913 - val_accuracy: 0.9603 - val_loss: 0.1378\n",
      "Epoch 36/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9568 - loss: 0.0987 - val_accuracy: 0.9663 - val_loss: 0.1190\n",
      "Epoch 37/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9597 - loss: 0.1004 - val_accuracy: 0.9544 - val_loss: 0.1190\n",
      "Epoch 38/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.0927 - val_accuracy: 0.9524 - val_loss: 0.1090\n",
      "Epoch 39/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9611 - loss: 0.0786 - val_accuracy: 0.9643 - val_loss: 0.1145\n",
      "Epoch 40/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9653 - loss: 0.0932 - val_accuracy: 0.9782 - val_loss: 0.0864\n",
      "Epoch 41/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9697 - loss: 0.0775 - val_accuracy: 0.9524 - val_loss: 0.1198\n",
      "Epoch 42/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.0916 - val_accuracy: 0.9683 - val_loss: 0.1320\n",
      "Epoch 43/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9589 - loss: 0.0931 - val_accuracy: 0.9702 - val_loss: 0.0927\n",
      "Epoch 44/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9655 - loss: 0.0703 - val_accuracy: 0.9643 - val_loss: 0.1118\n",
      "Epoch 45/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9677 - loss: 0.0918 - val_accuracy: 0.9544 - val_loss: 0.1273\n",
      "Epoch 46/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.0783 - val_accuracy: 0.9663 - val_loss: 0.0991\n",
      "Epoch 47/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1058 - val_accuracy: 0.9623 - val_loss: 0.1118\n",
      "Epoch 48/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.0773 - val_accuracy: 0.9583 - val_loss: 0.1245\n",
      "Epoch 49/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9630 - loss: 0.0772 - val_accuracy: 0.9583 - val_loss: 0.1420\n",
      "Epoch 50/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9566 - loss: 0.0878 - val_accuracy: 0.9563 - val_loss: 0.1455\n",
      "Epoch 51/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.0862 - val_accuracy: 0.9444 - val_loss: 0.1452\n",
      "Epoch 52/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.0791 - val_accuracy: 0.9663 - val_loss: 0.1150\n",
      "Epoch 53/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9643 - loss: 0.0799 - val_accuracy: 0.9683 - val_loss: 0.1192\n",
      "Epoch 54/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9687 - loss: 0.0751 - val_accuracy: 0.9524 - val_loss: 0.1140\n",
      "Epoch 55/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9670 - loss: 0.0805 - val_accuracy: 0.9385 - val_loss: 0.1321\n",
      "Epoch 56/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9691 - loss: 0.0632 - val_accuracy: 0.9563 - val_loss: 0.1766\n",
      "Epoch 57/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.0721 - val_accuracy: 0.9623 - val_loss: 0.1078\n",
      "Epoch 58/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0714 - val_accuracy: 0.9544 - val_loss: 0.1410\n",
      "Epoch 59/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.0785 - val_accuracy: 0.9524 - val_loss: 0.1509\n",
      "Epoch 60/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0777 - val_accuracy: 0.9623 - val_loss: 0.1084\n",
      "Epoch 61/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9640 - loss: 0.0985 - val_accuracy: 0.9603 - val_loss: 0.1228\n",
      "Epoch 62/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9718 - loss: 0.0699 - val_accuracy: 0.9524 - val_loss: 0.1701\n",
      "Epoch 63/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.0825 - val_accuracy: 0.9583 - val_loss: 0.1225\n",
      "Epoch 64/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.0723 - val_accuracy: 0.9643 - val_loss: 0.1039\n",
      "Epoch 65/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.0681 - val_accuracy: 0.9643 - val_loss: 0.1230\n",
      "Epoch 66/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0801 - val_accuracy: 0.9683 - val_loss: 0.1301\n",
      "Epoch 67/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9703 - loss: 0.0753 - val_accuracy: 0.9583 - val_loss: 0.1198\n",
      "Epoch 68/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9742 - loss: 0.0671 - val_accuracy: 0.9663 - val_loss: 0.1202\n",
      "Epoch 69/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.0784 - val_accuracy: 0.9643 - val_loss: 0.1306\n",
      "Epoch 70/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.0677 - val_accuracy: 0.9702 - val_loss: 0.1307\n",
      "Epoch 71/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9663 - loss: 0.0761 - val_accuracy: 0.9663 - val_loss: 0.1303\n",
      "Epoch 72/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.0631 - val_accuracy: 0.9722 - val_loss: 0.1218\n",
      "Epoch 73/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9679 - loss: 0.0691 - val_accuracy: 0.9623 - val_loss: 0.1327\n",
      "Epoch 74/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.0646 - val_accuracy: 0.9524 - val_loss: 0.1827\n",
      "Epoch 75/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.0798 - val_accuracy: 0.9623 - val_loss: 0.1306\n",
      "Epoch 76/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.0591 - val_accuracy: 0.9623 - val_loss: 0.1010\n",
      "Epoch 77/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.0645 - val_accuracy: 0.9563 - val_loss: 0.1316\n",
      "Epoch 78/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.0610 - val_accuracy: 0.9603 - val_loss: 0.1452\n",
      "Epoch 79/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9644 - loss: 0.0660 - val_accuracy: 0.9702 - val_loss: 0.1131\n",
      "Epoch 80/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9729 - loss: 0.0557 - val_accuracy: 0.9583 - val_loss: 0.1295\n",
      "Epoch 81/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9740 - loss: 0.0566 - val_accuracy: 0.9683 - val_loss: 0.1289\n",
      "Epoch 82/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.0641 - val_accuracy: 0.9683 - val_loss: 0.0988\n",
      "Epoch 83/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.0593 - val_accuracy: 0.9583 - val_loss: 0.1561\n",
      "Epoch 84/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9588 - loss: 0.0840 - val_accuracy: 0.9583 - val_loss: 0.1344\n",
      "Epoch 85/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9690 - loss: 0.0703 - val_accuracy: 0.9603 - val_loss: 0.1661\n",
      "Epoch 86/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.0647 - val_accuracy: 0.9683 - val_loss: 0.1408\n",
      "Epoch 87/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9764 - loss: 0.0602 - val_accuracy: 0.9643 - val_loss: 0.1456\n",
      "Epoch 88/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9712 - loss: 0.0719 - val_accuracy: 0.9762 - val_loss: 0.1225\n",
      "Epoch 89/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.0563 - val_accuracy: 0.9683 - val_loss: 0.1544\n",
      "Epoch 90/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0629 - val_accuracy: 0.9643 - val_loss: 0.1436\n",
      "Epoch 91/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0601 - val_accuracy: 0.9603 - val_loss: 0.1715\n",
      "Epoch 92/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.0703 - val_accuracy: 0.9623 - val_loss: 0.1206\n",
      "Epoch 93/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0508 - val_accuracy: 0.9702 - val_loss: 0.1155\n",
      "Epoch 94/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.0663 - val_accuracy: 0.9583 - val_loss: 0.1478\n",
      "Epoch 95/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0681 - val_accuracy: 0.9623 - val_loss: 0.1316\n",
      "Epoch 96/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9593 - loss: 0.0752 - val_accuracy: 0.9603 - val_loss: 0.1369\n",
      "Epoch 97/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0461 - val_accuracy: 0.9603 - val_loss: 0.1585\n",
      "Epoch 98/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9750 - loss: 0.0506 - val_accuracy: 0.9583 - val_loss: 0.1639\n",
      "Epoch 99/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.0640 - val_accuracy: 0.9643 - val_loss: 0.1398\n",
      "Epoch 100/100\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9732 - loss: 0.0586 - val_accuracy: 0.9663 - val_loss: 0.1289\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy on test set: 96.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "model = Sequential()\n",
    "model.add(Dense(123, input_shape=(x_train.shape[1],), activation=\"relu\"))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(56, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))  \n",
    "model.add(Dense(8, activation=\"relu\"))   \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))                               # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"RMSprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecf7e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = (model.predict(X_train_res) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea512e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ffb87a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "TRAINING SET EVALUATION\n",
      "[[2056   53]\n",
      " [  34 2075]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      2109\n",
      "           1       0.98      0.98      0.98      2109\n",
      "\n",
      "    accuracy                           0.98      4218\n",
      "   macro avg       0.98      0.98      0.98      4218\n",
      "weighted avg       0.98      0.98      0.98      4218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = (model.predict(X_train_res) > 0.5).astype(int)\n",
    "\n",
    "# TRAINING SET EVALUATION\n",
    "print(\"TRAINING SET EVALUATION\")\n",
    "print(confusion_matrix(y_train_res, y_train_pred))  # Pass the true labels as the first argument\n",
    "print(classification_report(y_train_res, y_train_pred))  # Compare true vs predicted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19716515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
